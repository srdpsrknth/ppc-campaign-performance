# PPC Campaign Performance Analytics

# PPC Campaign Performance Analytics

## Project Overview
This project analyzes real-world Pay-Per-Click (PPC) campaign performance data with a focus on **data validation, metric reliability, and performance distribution**, rather than surface-level KPI reporting. The goal is to demonstrate how a mid–senior analyst approaches marketing data before making optimization or budget-allocation decisions.

The analysis emphasizes:
- Understanding what the data truly represents
- Verifying how key marketing metrics are calculated
- Evaluating skew, concentration, and variability in performance

---

## Dataset
**Source:** Kaggle – PPC Campaign Performance Dataset  
**Size:** 1,000 rows, 954 unique campaigns

Each row represents a **campaign-level performance snapshot at a given reporting date**. While some campaigns appear on multiple dates (up to three), the dataset is best treated as a set of campaign snapshots rather than a complete daily time series.

---

## Step 1 — Data Validation & Performance Profiling

### Dataset Structure & Metric Definitions
- The effective grain of the data is **(campaign_id, date)**
- Multiple performance metrics are provided alongside raw fields (impressions, clicks, spend, revenue)
- Metric reconciliation shows:
  - **ROAS** is consistent with `revenue / spend`
  - **CPC** is calculated as `budget / clicks` (not spend-based)
  - **CTR** shows small discrepancies versus `clicks / impressions`, likely due to rounding or alternative logic

**Analytical decision:** Core performance metrics are recomputed from raw fields for consistency, while provided metrics are retained for reference.

---

### Distribution, Skew & Concentration

#### Metric Distributions
- **Revenue** and **Conversions** are strongly right-skewed
- **Spend** and **Clicks** are approximately symmetric

This indicates that averages for outcome metrics are heavily influenced by a small number of high-performing campaigns.

#### Concentration of Outcomes
Share of totals generated by the top 5% of campaigns:
- Revenue: ~23%
- Conversions: ~18%
- Clicks: ~11%
- Spend: ~10%

Outcomes are substantially more concentrated than investment, highlighting uneven campaign efficiency.

---

### Outliers & Efficiency Mismatch
- Only one campaign appears in both the top-spend and top-revenue groups
- High spend does not reliably translate into high revenue

This suggests potential misallocation risk and motivates efficiency-based optimization rather than scale-based budgeting.

---

### Platform-Level Variability
- **Spend variability** is highest on Instagram
- **Revenue variability** is highest on Facebook

Large spreads between median and upper percentiles indicate that platform averages mask significant risk and heterogeneity.

---

### Platform Efficiency (Recomputed Metrics)
- Highest average ROAS: Facebook
- Lowest spend-based CPC: LinkedIn
- Highest conversion rate: Facebook

Given the observed skew, these averages should be interpreted cautiously; median and percentile-based metrics provide more reliable guidance than means alone.

---

## Repository Structure
```text
ppc-campaign-performance/
├── data/
│   └── raw/                # raw CSV (not committed)
├── scripts/
│   └── load_to_sqlite.py   # ingestion script
├── sql/
│   ├── staging/            # data validation & canonical metrics
│   ├── intermediate/       # campaign aggregation & segmentation
│   └── marts/              # scenario and what-if analysis
├── notebooks/              # exploratory analysis (optional)
├── requirements.txt
└── README.md
```

---

## Analytical Principles Demonstrated

* Metric provenance and validation
* Robust statistics (medians, percentiles) over naive averages
* Skew and concentration analysis
* Campaign- and platform-level efficiency assessment
* Analytics-engineering-style SQL organization

---

## Campaign-Level Performance & Segmentation

Building on the validated staging layer, campaign performance is aggregated across all available snapshots to create a decision-ready view at the campaign level. Metrics are computed from summed numerators and denominators (rather than averaging pre-calculated rates) to ensure mathematical correctness.

### Campaign Performance Aggregation

* Campaigns are aggregated across all appearances
* Canonical metrics (CTR, CVR, CPC, CPA, ROAS) are recomputed from totals
* This avoids common pitfalls such as averaging ratios or double-counting

### Efficiency Segmentation

Campaigns are segmented using percentile-based thresholds on ROAS, CPA, and spend to identify clear action categories:

* **Scale (high ROAS, low CPA):** Efficient, underfunded campaigns suitable for immediate budget increases
* **Promising (high ROAS, low spend):** Strong performers with growth potential
* **Waste (high spend, low ROAS):** Primary sources of inefficiency and candidates for budget reduction
* **Low impact:** Low spend and low return campaigns with limited upside
* **Monitor:** Middle-of-the-pack campaigns requiring observation rather than immediate action

### Key Findings

* ~14–15% of campaigns fall into the **Scale** segment, delivering very high ROAS at below-average spend
* ~19–20% of campaigns are classified as **Waste**, absorbing the highest spend while delivering poor returns
* Only a small overlap exists between top-spend and top-performing campaigns, indicating inefficient allocation

This segmentation framework enables clear prioritization of optimization efforts and sets the foundation for budget reallocation analysis.

---

## Budget Reallocation & What-If Impact

This step evaluates how reallocating existing budget—without increasing total spend—could improve overall efficiency and revenue.

### Methodology

* Identify **reallocatable spend** based on observed performance (not arbitrary percentages)
* Remove spend *and observed revenue* from underperforming campaigns
* Reinvest freed budget only into proven high-efficiency segments
* Assume **constant ROAS within segments** (no performance lift), making results conservative

Two portfolio-level scenarios are evaluated.

### Scenario A — Median ROAS Reallocation (Upper-Bound Potential)

* Donors: campaigns with ROAS below the median (ROAS ≈ 7.30)
* Reallocated spend: ~53.6% of total budget
* Recipients: **Scale** and **Promising** segments
* Allocation weights proportional to segment-level ROAS

**Result:**

* Blended ROAS improves from **10.05 → 24.02**
* Total revenue increases from **~59.9M → ~143.1M** at constant spend

This represents an aggressive, upper-bound scenario highlighting the magnitude of inefficiency in the current allocation.

### Scenario B — Bottom Quintile Reallocation (Conservative)

* Donors: worst-performing 20% of campaigns by ROAS
* Smaller reallocation pool focused on clear underperformers

**Result:**

* Blended ROAS improves to **16.30**
* Total revenue increases to **~97.1M** at constant spend

This scenario reflects a more realistic first step for most organizations.

### Key Takeaway

Even under conservative assumptions, reallocating spend away from underperforming campaigns toward proven high-efficiency segments can materially improve revenue and efficiency without increasing total budget. The range between the two scenarios provides a practical decision envelope rather than a single point estimate.

---

## Platform-Level Budget Reallocation

In addition to portfolio-wide optimization, budget reallocation is evaluated **within each platform independently**, keeping total spend per platform constant. This reflects real-world constraints where platforms serve different roles and budgets are often managed at the channel level.

### Rationale

* Platforms are not fully interchangeable (e.g., search vs social vs video)
* Channel managers typically optimize **within-platform** budgets
* Platform-level optimization reduces execution risk while remaining highly actionable

### Methodology

For each platform:

* Compute the **platform-specific median ROAS**
* Treat campaigns below that median as the reallocatable pool
* Remove their observed spend and revenue
* Reinvest freed budget only into **Scale** and **Promising** campaigns on the same platform
* Allocate reinvestment proportionally to segment-level ROAS

No assumptions are made about performance improvement; ROAS is assumed to remain constant within each segment.

### Platform-Level Results Summary

| Platform  | Baseline ROAS | Reallocated ROAS | Revenue Uplift |
| --------- | ------------- | ---------------- | -------------- |
| Facebook  | 11.22         | 24.90            | +16.9M         |
| Google    | 10.43         | 23.57            | +14.4M         |
| Instagram | 9.53          | 23.91            | +16.7M         |
| LinkedIn  | 9.96          | 23.63            | +18.4M         |
| YouTube   | 9.05          | 23.89            | +16.6M         |

### Key Insights

* **LinkedIn** shows the largest absolute revenue uplift, driven by high internal inefficiency and strong high-performing campaigns
* **Google** benefits the least, reflecting a more efficient baseline allocation and lower optimization headroom
* Significant efficiency gains are achievable **without shifting budget across platforms**

This platform-level view complements the global reallocation scenarios and provides a lower-risk, more operationally realistic optimization path.


