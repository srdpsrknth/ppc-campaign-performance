# PPC Campaign Performance Analytics

## Project Overview

This project analyzes real-world Pay-Per-Click (PPC) campaign performance data with a focus on **data validation, metric reliability, and performance distribution**, rather than surface-level KPI reporting. The goal is to demonstrate how a mid–senior analyst approaches marketing data before making optimization or budget-allocation decisions.

The analysis emphasizes:

* Understanding what the data truly represents
* Verifying how key marketing metrics are calculated
* Evaluating skew, concentration, and variability in performance

---

## Dataset

**Source:** Kaggle – PPC Campaign Performance Dataset
**Size:** 1,000 rows, 954 unique campaigns

Each row represents a **campaign-level performance snapshot at a given reporting date**. While some campaigns appear on multiple dates (up to three), the dataset is best treated as a set of campaign snapshots rather than a complete daily time series.

---

## Step 1 — Data Validation & Performance Profiling

### Dataset Structure & Metric Definitions

* The effective grain of the data is **(campaign_id, date)**
* Multiple performance metrics are provided alongside raw fields (impressions, clicks, spend, revenue)
* Metric reconciliation shows:

  * **ROAS** is consistent with `revenue / spend`
  * **CPC** is calculated as `budget / clicks` (not spend-based)
  * **CTR** shows small discrepancies versus `clicks / impressions`, likely due to rounding or alternative logic

**Analytical decision:** Core performance metrics are recomputed from raw fields for consistency, while provided metrics are retained for reference.

---

### Distribution, Skew & Concentration

#### Metric Distributions

* **Revenue** and **Conversions** are strongly right-skewed
* **Spend** and **Clicks** are approximately symmetric

This indicates that averages for outcome metrics are heavily influenced by a small number of high-performing campaigns.

#### Concentration of Outcomes

Share of totals generated by the top 5% of campaigns:

* Revenue: ~23%
* Conversions: ~18%
* Clicks: ~11%
* Spend: ~10%

Outcomes are substantially more concentrated than investment, highlighting uneven campaign efficiency.

---

### Outliers & Efficiency Mismatch

* Only one campaign appears in both the top-spend and top-revenue groups
* High spend does not reliably translate into high revenue

This suggests potential misallocation risk and motivates efficiency-based optimization rather than scale-based budgeting.

---

### Platform-Level Variability

* **Spend variability** is highest on Instagram
* **Revenue variability** is highest on Facebook

Large spreads between median and upper percentiles indicate that platform averages mask significant risk and heterogeneity.

---

### Platform Efficiency (Recomputed Metrics)

* Highest average ROAS: Facebook
* Lowest spend-based CPC: LinkedIn
* Highest conversion rate: Facebook

Given the observed skew, these averages should be interpreted cautiously; median and percentile-based metrics provide more reliable guidance than means alone.

---

## Repository Structure

```text
ppc-campaign-performance/
├── data/
│   └── raw/                # raw CSV (not committed)
├── scripts/
│   └── load_to_sqlite.py   # ingestion script
├── sql/
│   └── staging/            # data validation & profiling SQL
├── notebooks/              # exploratory analysis (later)
├── requirements.txt
└── README.md
```

---

## Analytical Principles Demonstrated

* Metric provenance and validation
* Robust statistics (medians, percentiles) over naive averages
* Skew and concentration analysis
* Campaign- and platform-level efficiency assessment
* Analytics-engineering-style SQL organization

---

## Next Steps

* Build canonical staging models with standardized metrics
* Perform campaign and platform optimization analysis
* Develop visual summaries to support budget-allocation decisions
